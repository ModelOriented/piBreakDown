{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PW\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('D:\\\\Projects\\\\datasets\\\\blood-transfusion-service-center.csv')\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(data.loc[:, data.columns != 'Class'], data['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class piBreakDown:\n",
    "    \"\"\"\n",
    "    Python version of iBreakDown package in R (https://github.com/ModelOriented/iBreakDown)\n",
    "    \"\"\"\n",
    "    def __init__(self, model, data, target_label):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        model: scikit-learn model\n",
    "            a model to be explained, with `fit` and `predict` functions\n",
    "        data: pandas.DataFrame\n",
    "            data that was used to train model\n",
    "        target_label: str\n",
    "            label of target variable\n",
    "        \"\"\"\n",
    "        self._model = model\n",
    "        self._data = data\n",
    "        self._target_label = target_label\n",
    "        \n",
    "    def local_attributions(self, new_observation, keep_distributions = False, classes_names = None, order = None): \n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        new_observation: pandas.Series\n",
    "            a new observation with columns that correspond to variables used in the model\n",
    "        keep_distributions: boolean\n",
    "            if `True`, then distribution of partial predictions is stored\n",
    "        classes_names: list\n",
    "            names of the classes to be predicted, if `None` then it will be number from 0 to len(predicted values)\n",
    "        order: list\n",
    "            if not `None`, then it will be a fixed order of variables. It can be a numeric vector or vector \n",
    "            with names of variables\n",
    "        \"\"\"\n",
    "        cols_to_use = set(self._data.columns[self._data.columns != self._target_label]).intersection(set(new_observation.index))\n",
    "        target_yhat = self._model.predict_proba(new_observation.loc[cols_to_use].values.reshape(1,-1))[0]\n",
    "        if classes_names is None:\n",
    "            classes_names = list(range(0,len(target_yhat)))\n",
    "            \n",
    "        yhatpred = self._model.predict_proba(self._data.loc[:,self._data.columns != self._target_label])\n",
    "        baseline_yhat = yhatpred.mean(axis = 0)\n",
    "        average_yhats = self._calculated_1d_changes(self._data.loc[:, cols_to_use], new_observation[cols_to_use], classes_names)\n",
    "        diffs_1d = (average_yhats.subtract(baseline_yhat)**2).mean(axis = 1)\n",
    "        feature_path = self._create_ordered_path(diffs_1d, order)\n",
    "        \n",
    "        tmp = self._calculate_contributions_along_path(self._data.loc[:,self._data.columns != self._target_label],\n",
    "                                                      new_observation, feature_path, keep_distributions, self._target_label,\n",
    "                                                      baseline_yhat, target_yhat, classes_names)\n",
    "        return tmp\n",
    "    \n",
    "    def _calculated_1d_changes(self, data, new_observation, classes_names):\n",
    "        average_predictions_df = pd.DataFrame(columns=classes_names, index = data.columns)\n",
    "        for col in average_predictions_df.index:\n",
    "            data_tmp = data.copy()\n",
    "            data_tmp.loc[:,col] = new_observation.loc[col]\n",
    "            average_predictions_df.loc[col,:] =  self._model.predict_proba(data_tmp).mean(axis = 0)\n",
    "            \n",
    "        return average_predictions_df\n",
    "    \n",
    "    def _create_ordered_path(self, diffs_1d, order):\n",
    "        feature_path = pd.DataFrame({'diffs': diffs_1d})\n",
    "        if order is None:\n",
    "            feature_path = feature_path.sort_values(by = 'diffs', ascending = False)\n",
    "        else:\n",
    "            feature_path = feature_path.loc[order]\n",
    "            \n",
    "        return feature_path\n",
    "    \n",
    "    def _calculate_contributions_along_path(self, data, new_observation, feature_path, keep_distributions, label, baseline_yhat, target_yhat, classes_names):\n",
    "        open_variables = data.columns\n",
    "        current_data = data.copy()\n",
    "        step = 0\n",
    "        yhats = None\n",
    "        yhats_mean = pd.DataFrame(columns=classes_names, index=feature_path.index)\n",
    "        selected_rows = []\n",
    "\n",
    "        for i in feature_path.index:\n",
    "            candidates = [i]\n",
    "            if all([x in open_variables for x in candidates]):\n",
    "                current_data.loc[:,candidates] = new_observation[candidates].tolist()\n",
    "                step += 1\n",
    "                yhats_pred = self._model.predict_proba(current_data)\n",
    "                #if(keep_distributions):\n",
    "                    #TODO\n",
    "                    #distribution_for_batch\n",
    "                    \n",
    "                yhats_mean.loc[i,:] = yhats_pred.mean(axis = 0)\n",
    "                selected_rows.append(i)\n",
    "                open_variables = set(open_variables) - set(candidates)\n",
    "        selected = feature_path.loc[selected_rows,:]\n",
    "        selected_values = []\n",
    "        for i in selected.index:\n",
    "            selected_values.append(self._nice_pair(new_observation, i, None))\n",
    "            \n",
    "        variable_name = ['intercept'] + feature_path.index\n",
    "        variable_value = ['1'] + selected_values\n",
    "        variable = ['intercept'] + [x + ' = ' + y for x,y in zip(variable_name,selected_values)] + ['prediction']\n",
    "        cummulative = pd.DataFrame(columns=classes_names)\n",
    "        cummulative.loc['baseline_yhat',:] = baseline_yhat\n",
    "        cummulative = cummulative.append(yhats_mean)\n",
    "        cummulative.loc['target_yhat',:] = target_yhat\n",
    "        contribution = cummulative.diff(axis = 0)\n",
    "        contribution.loc['baseline_yhat',:] = cummulative.loc['baseline_yhat',:]\n",
    "        contribution.loc['target_yhat',:] = cummulative.loc['target_yhat',:]\n",
    "        \n",
    "        results = {}\n",
    "        results['variable_name'] = variable_name\n",
    "        results['variable_value'] = variable_value\n",
    "        results['variable'] = variable\n",
    "        results['cummulative'] = cummulative\n",
    "        results['contribution'] = contribution\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _nice_pair(self, x, ind1, ind2):\n",
    "        if(ind2 is None):\n",
    "            return self._nice_format(x[ind1])\n",
    "        return self._nice_format(x[ind1]) + ':' + self._nice_format(x[ind2])\n",
    "        \n",
    "    def _nice_format(self, x):\n",
    "        if type(x) in [int, float]:\n",
    "            return str(round(x,2))\n",
    "        return str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'variable_name': Index(['interceptV3', 'interceptV4', 'interceptV1', 'interceptV2'], dtype='object'),\n",
       " 'variable_value': ['1', '12500', '98', '2', '50'],\n",
       " 'variable': ['intercept',\n",
       "  'interceptV3 = 12500',\n",
       "  'interceptV4 = 98',\n",
       "  'interceptV1 = 2',\n",
       "  'interceptV2 = 50',\n",
       "  'prediction'],\n",
       " 'cummulative':                       0         1\n",
       " baseline_yhat  0.756295  0.243705\n",
       " V3             0.544089  0.455911\n",
       " V4             0.823262  0.176738\n",
       " V1              0.75361   0.24639\n",
       " V2                  0.1       0.9\n",
       " target_yhat         0.8       0.2,\n",
       " 'contribution':                        0          1\n",
       " baseline_yhat   0.756295   0.243705\n",
       " V3             -0.212206   0.212206\n",
       " V4              0.279173  -0.279173\n",
       " V1            -0.0696524  0.0696524\n",
       " V2              -0.65361    0.65361\n",
       " target_yhat          0.8        0.2}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd = piBreakDown(rf, data, 'Class')\n",
    "df = bd.local_attributions(data.loc[0,data.columns != 'Class'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class IBreakDown in module __main__:\n",
      "\n",
      "class IBreakDown(builtins.object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model, data, label)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  local_attributions(self, new_observation, keep_distributions=False, classes_names=None, order=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(IBreakDown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
